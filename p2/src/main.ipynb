{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install necessary packages\n",
    "%pip install -q torch numpy pandas transformers peft pyarrow pybind11 pylatexenc datasets tiktoken wandb tqdm matplotlib math-verify[antlr4_9_3]\n",
    "%pip install -q --upgrade --force-reinstall scikit-learn # -q 使用静默模式安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在下载Hugging Face模型前配置镜像站，防止AutoDL等实例无法访问hugging face官网\n",
    "import os     #Python 的内置os模块，用于操作操作系统的环境变量、文件路径等系统相关功能\n",
    "# HF_ENDPOINT是 Hugging Face 库的环境变量配置项，用于指定模型 / 数据集的下载端点（即服务器地址）\n",
    "# 将Hugging Face的默认地址从 https://huggingface.co（官网）改为 https://hf-mirror.com\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'     \n",
    "# 这是进程级别的环境变量，程序运行期间有效，程序退出后自动消失\n",
    "# 不会修改操作系统级别的环境变量配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  3 16:00:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.76.05              Driver Version: 580.76.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5090        On  |   00000000:B8:00.0 Off |                  N/A |\n",
      "| 42%   28C    P8             13W /  575W |       0MiB /  32607MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # GPU configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: 'Qwen/Qwen2.5-Math-1.5B'\n",
      "tokenizer_config.json: 7.32kB [00:00, 12.3MB/s]\n",
      "vocab.json: 2.78MB [00:01, 2.04MB/s]\n",
      "merges.txt: 1.67MB [00:00, 3.04MB/s]\n",
      "tokenizer.json: 7.03MB [00:00, 30.4MB/s]\n",
      "Loading dataset: 'ricdomolm/MATH-500'\n",
      "README.md: 1.80kB [00:00, 5.27MB/s]\n",
      "train-00000-of-00001.parquet: 100%|████████| 4.72M/4.72M [00:02<00:00, 2.24MB/s]\n",
      "test-00000-of-00001.parquet: 100%|████████████| 199k/199k [00:00<00:00, 627kB/s]\n",
      "Generating train split: 100%|██| 12000/12000 [00:00<00:00, 240464.61 examples/s]\n",
      "Generating test split: 100%|████████| 500/500 [00:00<00:00, 59920.34 examples/s]\n",
      "Dataset split into 10800 training and 1200 validation examples.\n",
      "\n",
      "Tokenizing and formatting datasets...\n",
      "Map (num_proc=208): 100%|█████████| 10800/10800 [00:26<00:00, 409.46 examples/s]\n",
      "Map (num_proc=208): 100%|████████████| 1200/1200 [00:25<00:00, 47.01 examples/s]\n",
      "Filter (num_proc=208): 100%|█████| 10800/10800 [00:03<00:00, 2964.90 examples/s]\n",
      "Filter (num_proc=208): 100%|████████| 1200/1200 [00:03<00:00, 333.93 examples/s]\n",
      "Final training samples: 9010\n",
      "Final validation samples: 1018\n",
      "\n",
      "--- DATASET STATS ---\n",
      "Sample of first training example:\n",
      " <|im_start|>system\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<|im_end|>\n",
      "<|im_start|>user\n",
      "Let $f$ be a linear function with the properties that $f(1) \\le\n",
      "f(2)$, $f(3) \\ge f(4)$, and $f(5) = 5$.  Which of the following statements is true?\n",
      "\n",
      "A) $f(0) < 0$\n",
      "\n",
      "B) $f(0) = 0$\n",
      "\n",
      "C) $f(1) < f(0) < f(-1)$\n",
      "\n",
      "D) $f(0) = 5$\n",
      "\n",
      "E) $f(0) > 5$\n",
      "Please reason step by step, and put your final answer within \\boxed{}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Since $f$ is a linear function, it has the form $f(x) = mx + b$. Because $f(1) \\le f(2)$, we have $m\n",
      "\\ge 0$.  Similarly, $f(3) \\ge f(4)$ implies $m \\le 0$. Hence, $m = 0$, and $f$ is a constant function.  Thus, $f(0) =\n",
      "f(5) = 5$, which means $\\boxed{\\text{D}}$ is true.<|im_end|>\n",
      "\n",
      "\n",
      "Max sequence length: 511\n",
      "Average sequence length: 256.71\n",
      "\n",
      "Saving training data to '/root/data/train.pkl'...\n",
      "Saving validation data to '/root/data/val.pkl'...\n",
      "--- PREPARATION COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# !python prepare.py --debug # Debug with small-scale data\n",
    "!python prepare.py # formal training using the full dataset\n",
    "# 在 prepare.py 中可修改使用的分词器模型（要与微调时使用的模型一致）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the baseline model with different hyperparameter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认模式（最小日志） 不设置verbose\n",
    "# 详细模式（显示进度条）--verbose 2\n",
    "# 静默模式 --verbose 0\n",
    "\n",
    "# 在finetune.py中可修改使用的 based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于个人GPU算力极其有限，我们只能使用Qwen3-0.6B-Base等参数很少的模型进行微调\n",
    "大模型训练需要的算力及其巨大，没有个人或学校可以完成，只有科技巨头如Google、微软、Meta等有足够的算力资源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在绝大多数情况下，AdamW + LoRA的理论和实践表现都优于其他配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune.py --optimization_method \"sgd\" --learning_rate 1e-5 --num_epochs 2 --output_dir \"saves/sgd_lr1e-5_epoch2\" --experiment_name \"SGD_lr1e-5_epoch2\" --verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune.py --optimization_method \"sgd\" --learning_rate 2e-5 --num_epochs 2 --output_dir \"saves/sgd_lr2e-5_epoch2\" --experiment_name \"SGD_lr2e-5_epoch2\" --verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune.py --optimization_method \"sgd\" --learning_rate 5e-5 --num_epochs 2 --output_dir \"saves/sgd_lr5e-5_epoch2\" --experiment_name \"SGD_lr5e-5_epoch2\" --verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer from Qwen/Qwen2.5-Math-1.5B...\n",
      "Loaded 9010 examples from /root/data/train.pkl\n",
      "Loaded 1018 examples from /root/data/val.pkl\n",
      "Setting up optimizer: sgd\n",
      "Starting training...\n",
      "Total training steps: 1126\n",
      "\n",
      "--- Epoch 1/1 ---\n",
      "Step 50: Train Loss = 0.9826\n",
      "\n",
      "Step 50: Train Loss = 0.9826, Val Loss = 0.8007\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 100: Train Loss = 0.5262\n",
      "\n",
      "Step 100: Train Loss = 0.5262, Val Loss = 0.7998\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 150: Train Loss = 0.5815\n",
      "\n",
      "Step 150: Train Loss = 0.5815, Val Loss = 0.7997\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 200: Train Loss = 0.7079\n",
      "\n",
      "Step 200: Train Loss = 0.7079, Val Loss = 0.7991\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 250: Train Loss = 0.4779\n",
      "\n",
      "Step 250: Train Loss = 0.4779, Val Loss = 0.7988\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 300: Train Loss = 0.6675\n",
      "\n",
      "Step 300: Train Loss = 0.6675, Val Loss = 0.7982\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 350: Train Loss = 0.6891\n",
      "\n",
      "Step 350: Train Loss = 0.6891, Val Loss = 0.7980\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 400: Train Loss = 0.7764\n",
      "\n",
      "Step 400: Train Loss = 0.7764, Val Loss = 0.7977\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 450: Train Loss = 0.6929\n",
      "\n",
      "Step 450: Train Loss = 0.6929, Val Loss = 0.7972\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 500: Train Loss = 0.8790\n",
      "\n",
      "Step 500: Train Loss = 0.8790, Val Loss = 0.7968\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 550: Train Loss = 0.7098\n",
      "\n",
      "Step 550: Train Loss = 0.7098, Val Loss = 0.7963\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 600: Train Loss = 0.5998\n",
      "\n",
      "Step 600: Train Loss = 0.5998, Val Loss = 0.7961\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 650: Train Loss = 0.9220\n",
      "\n",
      "Step 650: Train Loss = 0.9220, Val Loss = 0.7959\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 700: Train Loss = 0.3309\n",
      "\n",
      "Step 700: Train Loss = 0.3309, Val Loss = 0.7958\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 750: Train Loss = 0.8489\n",
      "\n",
      "Step 750: Train Loss = 0.8489, Val Loss = 0.7955\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 800: Train Loss = 0.5712\n",
      "\n",
      "Step 800: Train Loss = 0.5712, Val Loss = 0.7953\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 850: Train Loss = 0.9852\n",
      "\n",
      "Step 850: Train Loss = 0.9852, Val Loss = 0.7952\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 900: Train Loss = 0.8811\n",
      "\n",
      "Step 900: Train Loss = 0.8811, Val Loss = 0.7950\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 950: Train Loss = 0.8874\n",
      "\n",
      "Step 950: Train Loss = 0.8874, Val Loss = 0.7948\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 1000: Train Loss = 0.8227\n",
      "\n",
      "Step 1000: Train Loss = 0.8227, Val Loss = 0.7948\n",
      "Step 1050: Train Loss = 0.9120\n",
      "\n",
      "Step 1050: Train Loss = 0.9120, Val Loss = 0.7946\n",
      "  ✓ New best! Saving model to /root/saves/sgd_lr1e-5_epoch1\n",
      "Step 1100: Train Loss = 1.1531\n",
      "\n",
      "Step 1100: Train Loss = 1.1531, Val Loss = 0.7947\n",
      "\n",
      "Training finished. Running one final evaluation...\n",
      "Final validation... Done\n",
      "\n",
      "Final Validation Loss = 0.7945\n",
      "  ✓ Final model is the best! Saving to /root/saves/sgd_lr1e-5_epoch1\n",
      "\n",
      "Plotting loss curves...\n",
      "Training loss curve saved to /root/saves/sgd_lr1e-5_epoch1/train_loss_SGD_lr1e-5_epoch1.png\n",
      "Validation loss curve saved to /root/saves/sgd_lr1e-5_epoch1/val_loss_SGD_lr1e-5_epoch1.png\n",
      "\n",
      "==================================================\n",
      "TRAINING SUMMARY\n",
      "==================================================\n",
      "Model: Qwen/Qwen2.5-Math-1.5B\n",
      "Optimizer: sgd\n",
      "Best Validation Loss: 0.7946\n",
      "Final Training Steps: 1126\n",
      "Loss data saved for 112 training points and 22 validation points\n",
      "Training Time: 833.5s (13.9min)\n",
      "Peak GPU memory: 11.03 GB\n",
      "==================================================\n",
      "Process complete. Best model is saved in /root/saves/sgd_lr1e-5_epoch1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --optimization_method \"sgd\" --learning_rate 1e-5 --num_epochs 1 --output_dir \"saves/sgd_lr1e-5_epoch1\" --experiment_name \"SGD_lr1e-5_epoch1\" --verbose 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer from Qwen/Qwen2.5-Math-1.5B...\n",
      "Loaded 9010 examples from /root/data/train.pkl\n",
      "Loaded 1018 examples from /root/data/val.pkl\n",
      "Setting up optimizer: adam\n",
      "Starting training...\n",
      "Total training steps: 1126\n",
      "\n",
      "--- Epoch 1/1 ---\n",
      "Step 50: Train Loss = 0.9385\n",
      "\n",
      "Step 50: Train Loss = 0.9385, Val Loss = 0.6868\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 100: Train Loss = 0.3779\n",
      "\n",
      "Step 100: Train Loss = 0.3779, Val Loss = 0.6765\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 150: Train Loss = 0.5394\n",
      "\n",
      "Step 150: Train Loss = 0.5394, Val Loss = 0.6700\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 200: Train Loss = 0.5531\n",
      "\n",
      "Step 200: Train Loss = 0.5531, Val Loss = 0.6648\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 250: Train Loss = 0.4065\n",
      "\n",
      "Step 250: Train Loss = 0.4065, Val Loss = 0.6607\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 300: Train Loss = 0.5296\n",
      "\n",
      "Step 300: Train Loss = 0.5296, Val Loss = 0.6568\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 350: Train Loss = 0.6123\n",
      "\n",
      "Step 350: Train Loss = 0.6123, Val Loss = 0.6534\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 400: Train Loss = 0.5113\n",
      "\n",
      "Step 400: Train Loss = 0.5113, Val Loss = 0.6498\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 450: Train Loss = 0.5992\n",
      "\n",
      "Step 450: Train Loss = 0.5992, Val Loss = 0.6468\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 500: Train Loss = 0.7185\n",
      "\n",
      "Step 500: Train Loss = 0.7185, Val Loss = 0.6444\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 550: Train Loss = 0.6114\n",
      "\n",
      "Step 550: Train Loss = 0.6114, Val Loss = 0.6419\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 600: Train Loss = 0.5194\n",
      "\n",
      "Step 600: Train Loss = 0.5194, Val Loss = 0.6398\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 650: Train Loss = 0.7387\n",
      "\n",
      "Step 650: Train Loss = 0.7387, Val Loss = 0.6374\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 700: Train Loss = 0.2417\n",
      "\n",
      "Step 700: Train Loss = 0.2417, Val Loss = 0.6357\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 750: Train Loss = 0.7055\n",
      "\n",
      "Step 750: Train Loss = 0.7055, Val Loss = 0.6341\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 800: Train Loss = 0.4149\n",
      "\n",
      "Step 800: Train Loss = 0.4149, Val Loss = 0.6326\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 850: Train Loss = 0.8867\n",
      "\n",
      "Step 850: Train Loss = 0.8867, Val Loss = 0.6313\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 900: Train Loss = 0.6150\n",
      "\n",
      "Step 900: Train Loss = 0.6150, Val Loss = 0.6301\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 950: Train Loss = 0.7462\n",
      "\n",
      "Step 950: Train Loss = 0.7462, Val Loss = 0.6293\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 1000: Train Loss = 0.6947\n",
      "\n",
      "Step 1000: Train Loss = 0.6947, Val Loss = 0.6281\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 1050: Train Loss = 0.8009\n",
      "\n",
      "Step 1050: Train Loss = 0.8009, Val Loss = 0.6268\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "Step 1100: Train Loss = 1.0805\n",
      "\n",
      "Step 1100: Train Loss = 1.0805, Val Loss = 0.6259\n",
      "  ✓ New best! Saving model to /root/saves/adam_lr1e-5_epoch1\n",
      "\n",
      "Training finished. Running one final evaluation...\n",
      "Final validation... Done\n",
      "\n",
      "Final Validation Loss = 0.6251\n",
      "  ✓ Final model is the best! Saving to /root/saves/adam_lr1e-5_epoch1\n",
      "\n",
      "Plotting loss curves...\n",
      "Training loss curve saved to /root/saves/adam_lr1e-5_epoch1/train_loss_Adam_lr1e-5_epoch1.png\n",
      "Validation loss curve saved to /root/saves/adam_lr1e-5_epoch1/val_loss_Adam_lr1e-5_epoch1.png\n",
      "\n",
      "==================================================\n",
      "TRAINING SUMMARY\n",
      "==================================================\n",
      "Model: Qwen/Qwen2.5-Math-1.5B\n",
      "Optimizer: adam\n",
      "Best Validation Loss: 0.6259\n",
      "Final Training Steps: 1126\n",
      "Loss data saved for 112 training points and 22 validation points\n",
      "Training Time: 850.7s (14.2min)\n",
      "Peak GPU memory: 16.76 GB\n",
      "==================================================\n",
      "Process complete. Best model is saved in /root/saves/adam_lr1e-5_epoch1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --optimization_method \"adam\" --learning_rate 1e-5 --num_epochs 1 --output_dir \"saves/adam_lr1e-5_epoch1\" --experiment_name \"Adam_lr1e-5_epoch1\" --verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune.py --optimization_method \"adam\" --learning_rate 2e-5 --num_epochs 1 --output_dir \"saves/adam_lr2e-5_epoch1\" --experiment_name \"Adam_lr2e-5_epoch1\" --verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune.py --optimization_method \"adam\" --learning_rate 5e-5 --num_epochs 1 --output_dir \"saves/adam_lr5e-5_epoch1\" --experiment_name \"Adam_lr5e-5_epoch1\" --verbose 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer from Qwen/Qwen2.5-Math-1.5B...\n",
      "config.json: 676B [00:00, 2.13MB/s]\n",
      "model.safetensors: 100%|███████████████████| 3.09G/3.09G [04:25<00:00, 11.7MB/s]\n",
      "generation_config.json: 138B [00:00, 428kB/s]\n",
      "Loaded 9010 examples from /root/data/train.pkl\n",
      "Loaded 1018 examples from /root/data/val.pkl\n",
      "Setting up optimizer: lora\n",
      "Setting up LoRA with rank=8\n",
      "Starting training...\n",
      "Total training steps: 1126\n",
      "\n",
      "--- Epoch 1/1 ---\n",
      "Step 50: Train Loss = 0.7190\n",
      "\n",
      "Step 50: Train Loss = 0.7190, Val Loss = 0.7631\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 100: Train Loss = 0.6921\n",
      "\n",
      "Step 100: Train Loss = 0.6921, Val Loss = 0.7206\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 150: Train Loss = 0.5609\n",
      "\n",
      "Step 150: Train Loss = 0.5609, Val Loss = 0.6974\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 200: Train Loss = 0.7454\n",
      "\n",
      "Step 200: Train Loss = 0.7454, Val Loss = 0.6872\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 250: Train Loss = 0.7649\n",
      "\n",
      "Step 250: Train Loss = 0.7649, Val Loss = 0.6825\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 300: Train Loss = 0.6941\n",
      "\n",
      "Step 300: Train Loss = 0.6941, Val Loss = 0.6786\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 350: Train Loss = 0.3949\n",
      "\n",
      "Step 350: Train Loss = 0.3949, Val Loss = 0.6759\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 400: Train Loss = 0.7048\n",
      "\n",
      "Step 400: Train Loss = 0.7048, Val Loss = 0.6734\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 450: Train Loss = 0.6982\n",
      "\n",
      "Step 450: Train Loss = 0.6982, Val Loss = 0.6712\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 500: Train Loss = 0.3873\n",
      "\n",
      "Step 500: Train Loss = 0.3873, Val Loss = 0.6693\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 550: Train Loss = 0.5723\n",
      "\n",
      "Step 550: Train Loss = 0.5723, Val Loss = 0.6676\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 600: Train Loss = 0.6401\n",
      "\n",
      "Step 600: Train Loss = 0.6401, Val Loss = 0.6658\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 650: Train Loss = 0.8425\n",
      "\n",
      "Step 650: Train Loss = 0.8425, Val Loss = 0.6646\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 700: Train Loss = 0.8005\n",
      "\n",
      "Step 700: Train Loss = 0.8005, Val Loss = 0.6630\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 750: Train Loss = 0.8926\n",
      "\n",
      "Step 750: Train Loss = 0.8926, Val Loss = 0.6623\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 800: Train Loss = 0.6269\n",
      "\n",
      "Step 800: Train Loss = 0.6269, Val Loss = 0.6609\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 850: Train Loss = 0.7874\n",
      "\n",
      "Step 850: Train Loss = 0.7874, Val Loss = 0.6599\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 900: Train Loss = 0.5719\n",
      "\n",
      "Step 900: Train Loss = 0.5719, Val Loss = 0.6590\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 950: Train Loss = 0.4331\n",
      "\n",
      "Step 950: Train Loss = 0.4331, Val Loss = 0.6583\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 1000: Train Loss = 0.4740\n",
      "\n",
      "Step 1000: Train Loss = 0.4740, Val Loss = 0.6571\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 1050: Train Loss = 0.5665\n",
      "\n",
      "Step 1050: Train Loss = 0.5665, Val Loss = 0.6565\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "Step 1100: Train Loss = 0.4748\n",
      "\n",
      "Step 1100: Train Loss = 0.4748, Val Loss = 0.6555\n",
      "  ✓ New best! Saving model to /root/saves/adamW_lr1e-5_epoch1\n",
      "\n",
      "Training finished. Running one final evaluation...\n",
      "Final validation... Done\n",
      "\n",
      "Final Validation Loss = 0.6553\n",
      "  ✓ Final model is the best! Saving to /root/saves/adamW_lr1e-5_epoch1\n",
      "\n",
      "Plotting loss curves...\n",
      "Training loss curve saved to /root/saves/adamW_lr1e-5_epoch1/train_loss_AdamW_lr1e-5_epoch1.png\n",
      "Validation loss curve saved to /root/saves/adamW_lr1e-5_epoch1/val_loss_AdamW_lr1e-5_epoch1.png\n",
      "\n",
      "==================================================\n",
      "TRAINING SUMMARY\n",
      "==================================================\n",
      "Model: Qwen/Qwen2.5-Math-1.5B\n",
      "Optimizer: lora\n",
      "Best Validation Loss: 0.6555\n",
      "Final Training Steps: 1126\n",
      "Loss data saved for 112 training points and 22 validation points\n",
      "Training Time: 1395.9s (23.3min)\n",
      "Peak GPU memory: 8.95 GB\n",
      "==================================================\n",
      "Process complete. Best model is saved in /root/saves/adamW_lr1e-5_epoch1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 在finetune.py中lora默认使用AdamW优化器\n",
    "!python finetune.py --optimization_method \"lora\" --learning_rate 1e-5 --num_epochs 1 --output_dir \"saves/adamW_lr1e-5_epoch1\" --lora_rank 8 --experiment_name \"AdamW_lr1e-5_epoch1\" --verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune.py --optimization_method \"lora\" --learning_rate 2e-5 --num_epochs 1 --output_dir \"saves/adamW_lr2e-5_epoch1\" --lora_rank 8 --experiment_name \"AdamW_lr2e-5_epoch1\" --verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune.py --optimization_method \"lora\" --learning_rate 5e-5 --num_epochs 1 --output_dir \"saves/adamW_lr5e-5_epoch1\" --lora_rank 8 --experiment_name \"AdamW_lr5e-5_epoch1\" --verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较sgd不同超参数配置的结果\n",
    "!python compare_experiments.py \\\n",
    "  --experiment_dirs \"saves/sgd_lr1e-5_epoch2\" \"saves/sgd_lr2e-5_epoch2\" \"saves/sgd_lr5e-5_epoch2\" \\\n",
    "  --output_dir \"sgd_lr_comparison_results\" \\\n",
    "  --title_suffix \" - SGD Learning Rate Comparison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较Adam不同超参数配置的结果\n",
    "!python compare_experiments.py \\\n",
    "  --experiment_dirs \"saves/adam_lr1e-5_epoch1\" \"saves/adam_lr2e-5_epoch1\" \"saves/adam_lr5e-5_epoch1\" \"saves/adam_lr1e-4_epoch1\" \"saves/adam_lr3e-4_epoch1\" \\\n",
    "  --output_dir \"comparison_results/adam_lr_comparison\" \\\n",
    "  --title_suffix \" - Adam Learning Rate Comparison (epoch=1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较AdamW + LoRA不同超参数配置的结果\n",
    "!python compare_experiments.py \\\n",
    "  --experiment_dirs \"saves/adamW_lr1e-5_epoch1\" \"saves/adamW_lr2e-5_epoch1\" \"saves/adamW_lr5e-5_epoch1\" \\\n",
    "  --output_dir \"comparison_results/adamW_lr_comparison\" \\\n",
    "  --title_suffix \" - AdamW Learning Rate Comparison (epoch=1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 3 experiments: ['saves/adamW_lr1e-5_epoch1', 'saves/adam_lr1e-5_epoch1', 'saves/sgd_lr1e-5_epoch1']\n",
      "Loaded data from 3 experiments\n",
      "Training loss comparison saved to comparison_results/optimizer_comparison/train_loss_comparison.png\n",
      "Validation loss comparison saved to comparison_results/optimizer_comparison/val_loss_comparison.png\n",
      "\n",
      "Comparison complete!\n",
      "Training comparison saved to: comparison_results/optimizer_comparison/train_loss_comparison.png\n",
      "Validation comparison saved to: comparison_results/optimizer_comparison/val_loss_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# 比较 sgd adam adamW在相同学习率下的结果\n",
    "!python compare_experiments.py \\\n",
    "  --experiment_dirs \"saves/adamW_lr1e-5_epoch1\" \"saves/adam_lr1e-5_epoch1\" \"saves/sgd_lr1e-5_epoch1\" \\\n",
    "  --output_dir \"comparison_results/optimizer_comparison\" \\\n",
    "  --title_suffix \" - Optimizer Comparison (epoch=1)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout the finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用模型对测试集进行推理，生成预测结果\n",
    "\n",
    "功能：\n",
    "加载测试数据集（MATH-500）\n",
    "加载指定的模型（基础模型或LoRA微调模型）\n",
    "对每个测试问题生成模型的回答\n",
    "将原始预测结果保存为JSONL格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python rollout.py --model \"Qwen/Qwen3-0.6B-Base\" --lora_path \"saves/lora-tuned\" --output_file \"output/qwen3_0.6b_base_nosys_it_lora_debug.jsonl\"\n",
    "!python rollout.py --model \"saves/sgd_lr1e-5_epoch2\" --output_file \"output/rolled/sgd_lr1e-5_epoch2.jsonl\"\n",
    "!python rollout.py --model \"saves/sgd_lr2e-5_epoch2\" --output_file \"output/rolled/sgd_lr2e-5_epoch2.jsonl\"\n",
    "!python rollout.py --model \"saves/sgd_lr5e-5_epoch2\" --output_file \"output/rolled/sgd_lr5e-5_epoch2.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rollout.py --model \"saves/adam_lr1e-5_epoch1\" --output_file \"output/rolled/adam_lr1e-5_epoch1.jsonl\"\n",
    "!python rollout.py --model \"saves/adam_lr2e-5_epoch1\" --output_file \"output/rolled/adam_lr2e-5_epoch1.jsonl\"\n",
    "!python rollout.py --model \"saves/adam_lr5e-5_epoch1\" --output_file \"output/rolled/adam_lr5e-5_epoch1.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rollout.py --model \"saves/adamW_lr1e-5_epoch1\" --output_file \"output/rolled/adamW_lr1e-5_epoch1.jsonl\"\n",
    "!python rollout.py --model \"saves/adamW_lr2e-5_epoch1\" --output_file \"output/rolled/adamW_lr2e-5_epoch1.jsonl\"\n",
    "!python rollout.py --model \"saves/adamW_lr5e-5_epoch1\" --output_file \"output/rolled/adamW_lr5e-5_epoch1.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: saves/sgd_lr1e-5_epoch1\n",
      "No LoRA adapter specified, running the base model.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 1/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 2/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 3/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 4/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 5/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 6/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 7/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 8/8\n",
      "Output will be saved to: /root/output/rolled/sgd_lr1e-5_epoch1.jsonl\n",
      "Saved generations to /root/output/rolled/sgd_lr1e-5_epoch1.jsonl\n",
      "Loading base model: saves/adam_lr1e-5_epoch1\n",
      "No LoRA adapter specified, running the base model.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 1/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 2/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 3/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 4/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 5/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 6/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 7/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 8/8\n",
      "Output will be saved to: /root/output/rolled/adam_lr1e-5_epoch1.jsonl\n",
      "Saved generations to /root/output/rolled/adam_lr1e-5_epoch1.jsonl\n",
      "Loading base model: saves/adamW_lr1e-5_epoch1\n",
      "No LoRA adapter specified, running the base model.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 1/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 2/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 3/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 4/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 5/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 6/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 7/8\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processed batch 8/8\n",
      "Output will be saved to: /root/output/rolled/adamW_lr1e-5_epoch1.jsonl\n",
      "Saved generations to /root/output/rolled/adamW_lr1e-5_epoch1.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python rollout.py --model \"saves/sgd_lr1e-5_epoch1\" --output_file \"output/rolled/sgd_lr1e-5_epoch1.jsonl\"\n",
    "!python rollout.py --model \"saves/adam_lr1e-5_epoch1\" --output_file \"output/rolled/adam_lr1e-5_epoch1.jsonl\"\n",
    "!python rollout.py --model \"saves/adamW_lr1e-5_epoch1\" --output_file \"output/rolled/adamW_lr1e-5_epoch1.jsonl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the rollout results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "对预测结果进行评分和统计分析\n",
    "1.读取 rollout.py 生成的JSONL文件\n",
    "2.使用 verifier.py 中的 compute_score 函数对每个预测评分\n",
    "3.计算整体准确率\n",
    "4.将评分结果添加到JSONL文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --input_file \"output/rolled/sgd_lr1e-5_epoch2.jsonl\" --output_file \"output/sgd_lr1e-5_epoch2_evaled.jsonl\"\n",
    "!python evaluate.py --input_file \"output/rolled/sgd_lr2e-5_epoch2.jsonl\" --output_file \"output/sgd_lr2e-5_epoch2_evaled.jsonl\"\n",
    "!python evaluate.py --input_file \"output/rolled/sgd_lr5e-5_epoch2.jsonl\" --output_file \"output/sgd_lr5e-5_epoch2_evaled.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --input_file \"output/rolled/adam_lr1e-5_epoch1.jsonl\" --output_file \"output/evaluated/adam_lr1e-5_epoch1_evaled.jsonl\"\n",
    "!python evaluate.py --input_file \"output/rolled/adam_lr2e-5_epoch1.jsonl\" --output_file \"output/evaluated/adam_lr2e-5_epoch1_evaled.jsonl\"\n",
    "!python evaluate.py --input_file \"output/rolled/adam_lr5e-5_epoch1.jsonl\" --output_file \"output/evaluated/adam_lr5e-5_epoch1_evaled.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --input_file \"output/rolled/adamW_lr1e-5_epoch1.jsonl\" --output_file \"output/evaluated/adamW_lr1e-5_epoch1_evaled.jsonl\"\n",
    "!python evaluate.py --input_file \"output/rolled/adamW_lr2e-5_epoch1.jsonl\" --output_file \"output/evaluated/adamW_lr2e-5_epoch1_evaled.jsonl\"\n",
    "!python evaluate.py --input_file \"output/rolled/adamW_lr5e-5_epoch1.jsonl\" --output_file \"output/evaluated/adamW_lr5e-5_epoch1_evaled.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete.\n",
      "Overall Accuracy: 11.80%\n",
      "Scored results saved to output/sgd_lr1e-5_epoch1_evaled.jsonl\n",
      "\n",
      "Evaluation complete.\n",
      "Overall Accuracy: 24.60%\n",
      "Scored results saved to output/evaluated/adam_lr1e-5_epoch1_evaled.jsonl\n",
      "\n",
      "Evaluation complete.\n",
      "Overall Accuracy: 26.20%\n",
      "Scored results saved to output/evaluated/adamW_lr1e-5_epoch1_evaled.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --input_file \"output/rolled/sgd_lr1e-5_epoch1.jsonl\" --output_file \"output/sgd_lr1e-5_epoch1_evaled.jsonl\"\n",
    "!python evaluate.py --input_file \"output/rolled/adam_lr1e-5_epoch1.jsonl\" --output_file \"output/evaluated/adam_lr1e-5_epoch1_evaled.jsonl\"\n",
    "!python evaluate.py --input_file \"output/rolled/adamW_lr1e-5_epoch1.jsonl\" --output_file \"output/evaluated/adamW_lr1e-5_epoch1_evaled.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
