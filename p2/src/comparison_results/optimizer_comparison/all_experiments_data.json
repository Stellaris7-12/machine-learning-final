[
  {
    "experiment_name": "AdamW_lr1e-5_epoch1",
    "train_losses": [
      1.0675349235534668,
      0.8110747933387756,
      0.672155499458313,
      0.6971362233161926,
      0.7190399765968323,
      0.8353633880615234,
      0.45427995920181274,
      0.9499494433403015,
      1.0885441303253174,
      0.6920732855796814,
      0.7733046412467957,
      0.8573057055473328,
      0.8204172849655151,
      0.7271141409873962,
      0.5609351396560669,
      0.8640718460083008,
      0.4940219223499298,
      0.6497339010238647,
      0.692950427532196,
      0.7454257607460022,
      0.9038001298904419,
      0.61518794298172,
      0.7085853815078735,
      0.558367908000946,
      0.7648919224739075,
      0.72879558801651,
      0.6196560263633728,
      0.48595988750457764,
      0.9892796277999878,
      0.6941347718238831,
      0.935135006904602,
      1.1695003509521484,
      0.6294394731521606,
      0.9435469508171082,
      0.394875705242157,
      0.6395580768585205,
      0.564857542514801,
      0.30369916558265686,
      0.5576800107955933,
      0.7047972679138184,
      0.9027971029281616,
      0.881176769733429,
      0.7312016487121582,
      0.411711722612381,
      0.6982212066650391,
      0.7368214130401611,
      0.745152473449707,
      0.6477762460708618,
      0.7924973368644714,
      0.38728412985801697,
      0.9651445746421814,
      0.7269632816314697,
      0.797408401966095,
      0.9499887228012085,
      0.5722640156745911,
      0.3893243670463562,
      1.002776026725769,
      0.6632235050201416,
      0.701250433921814,
      0.6400781273841858,
      0.4866568148136139,
      0.7729599475860596,
      0.9452800154685974,
      0.5912885069847107,
      0.8424659967422485,
      0.7240054607391357,
      0.5191063284873962,
      1.0074800252914429,
      0.430155873298645,
      0.8004551529884338,
      0.7172849774360657,
      0.6235142350196838,
      0.28562960028648376,
      1.051648736000061,
      0.8926228284835815,
      0.7819439768791199,
      0.993584394454956,
      0.45489951968193054,
      0.711818516254425,
      0.6269468069076538,
      0.5434290766716003,
      0.6116976737976074,
      0.6313432455062866,
      0.47163817286491394,
      0.7874127626419067,
      0.6869188547134399,
      0.9336462616920471,
      0.9646149277687073,
      0.5693817734718323,
      0.5718865394592285,
      0.5335484743118286,
      0.5692030191421509,
      0.58590167760849,
      0.7399182319641113,
      0.43312010169029236,
      0.6351559162139893,
      0.7096449136734009,
      0.8974541425704956,
      0.7924529314041138,
      0.47403064370155334,
      0.9013498425483704,
      0.5808213353157043,
      0.6905477046966553,
      0.6779773235321045,
      0.5664637088775635,
      0.35668522119522095,
      0.6164482831954956,
      0.5239519476890564,
      0.7011188268661499,
      0.47482582926750183,
      0.5608929991722107,
      0.9075887799263
    ],
    "train_steps": [
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000,
      1010,
      1020,
      1030,
      1040,
      1050,
      1060,
      1070,
      1080,
      1090,
      1100,
      1110,
      1120
    ],
    "val_losses": [
      0.7631034609259697,
      0.720582168556618,
      0.6973947103342045,
      0.6871653509807493,
      0.6825142898002168,
      0.6786058384390852,
      0.6759044771578551,
      0.6734446864465365,
      0.6712295745585428,
      0.6692657353601662,
      0.6675520739234501,
      0.66575149799494,
      0.6645534674111893,
      0.6630145420845459,
      0.6622646944579066,
      0.660858439848793,
      0.6599237823708821,
      0.6589928562725457,
      0.6582997262126571,
      0.6571204026754807,
      0.6565445093017196,
      0.6555207360001585
    ],
    "val_steps": [
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000,
      1050,
      1100
    ],
    "args": {
      "model_name": "Qwen/Qwen2.5-Math-1.5B",
      "data_dir": "data",
      "output_dir": "saves/adamW_lr1e-5_epoch1",
      "num_epochs": 1,
      "learning_rate": 1e-05,
      "weight_decay": 0.01,
      "beta1": 0.9,
      "beta2": 0.95,
      "batch_size": 2,
      "grad_accumulation_steps": 4,
      "log_interval": 10,
      "eval_interval": 50,
      "verbose": 1,
      "optimization_method": "lora",
      "lora_rank": 8,
      "experiment_name": "AdamW_lr1e-5_epoch1"
    }
  },
  {
    "experiment_name": "Adam_lr1e-5_epoch1",
    "train_losses": [
      0.8986225724220276,
      0.738908052444458,
      0.5558378100395203,
      0.6016831398010254,
      0.93852299451828,
      0.8532282710075378,
      0.44670453667640686,
      0.44959354400634766,
      0.9367928504943848,
      0.3778921663761139,
      0.4870744049549103,
      0.4233311712741852,
      0.4424421787261963,
      0.73333340883255,
      0.5393838882446289,
      0.5113791823387146,
      0.4226400554180145,
      0.7845630645751953,
      0.4926069676876068,
      0.5530874133110046,
      1.0941499471664429,
      0.49424657225608826,
      0.5108259916305542,
      0.7620171308517456,
      0.4064893424510956,
      0.7734677195549011,
      0.8684121370315552,
      0.6905273199081421,
      0.8824811577796936,
      0.5296247601509094,
      0.7934169769287109,
      0.6314318180084229,
      1.0432727336883545,
      0.9095355868339539,
      0.6123432517051697,
      0.4977805018424988,
      0.7746976613998413,
      0.6640622615814209,
      0.6408078670501709,
      0.5112662315368652,
      0.7548508048057556,
      0.9507516026496887,
      0.3063899874687195,
      0.9120789766311646,
      0.5991732478141785,
      0.7598900198936462,
      0.9397783875465393,
      0.5706630945205688,
      0.7763234376907349,
      0.7185187339782715,
      0.6162340641021729,
      0.47874921560287476,
      0.34598278999328613,
      0.5969216823577881,
      0.6114386916160583,
      1.0252189636230469,
      0.532721757888794,
      0.6529222130775452,
      0.7251183986663818,
      0.5194215774536133,
      0.6351792216300964,
      0.6948854327201843,
      1.3443048000335693,
      0.5935863852500916,
      0.7387470006942749,
      0.43379655480384827,
      0.31970658898353577,
      0.5322921872138977,
      0.21107202768325806,
      0.2417326271533966,
      0.43512779474258423,
      0.9773238897323608,
      0.8479076623916626,
      0.31782543659210205,
      0.7055274844169617,
      0.5271282196044922,
      0.29265815019607544,
      0.6588209867477417,
      0.6491238474845886,
      0.41488170623779297,
      0.3733879029750824,
      0.2900371551513672,
      0.4550155997276306,
      0.6128783226013184,
      0.8866652250289917,
      0.7553938627243042,
      0.576903760433197,
      0.8550457954406738,
      0.5321728587150574,
      0.615047812461853,
      0.6881928443908691,
      0.8029879927635193,
      0.9249674081802368,
      1.0835877656936646,
      0.7462226748466492,
      0.7323921322822571,
      0.6064667105674744,
      0.5512778759002686,
      0.9496175050735474,
      0.6946630477905273,
      0.47469502687454224,
      0.5554019808769226,
      0.7318844795227051,
      0.5389149188995361,
      0.8008644580841064,
      0.5696490406990051,
      0.5476516485214233,
      0.7675028443336487,
      0.5155271887779236,
      1.0804930925369263,
      0.4076167047023773,
      0.7158089280128479
    ],
    "train_steps": [
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000,
      1010,
      1020,
      1030,
      1040,
      1050,
      1060,
      1070,
      1080,
      1090,
      1100,
      1110,
      1120
    ],
    "val_losses": [
      0.6867642549611263,
      0.6765065155996555,
      0.6700035719482744,
      0.6648444472455323,
      0.6607109857155906,
      0.6568029176275482,
      0.6533513323848514,
      0.6497606595571945,
      0.6468365888581529,
      0.64438753547275,
      0.6418954332181653,
      0.6398274660227575,
      0.6374074799138577,
      0.6356504282571946,
      0.634052026833907,
      0.6326278350325136,
      0.6312803432550318,
      0.6300973687869861,
      0.6293086361264433,
      0.6281280013163573,
      0.6268128259125768,
      0.6259325063533071
    ],
    "val_steps": [
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000,
      1050,
      1100
    ],
    "args": {
      "model_name": "Qwen/Qwen2.5-Math-1.5B",
      "data_dir": "data",
      "output_dir": "saves/adam_lr1e-5_epoch1",
      "num_epochs": 1,
      "learning_rate": 1e-05,
      "weight_decay": 0.01,
      "beta1": 0.9,
      "beta2": 0.95,
      "batch_size": 2,
      "grad_accumulation_steps": 4,
      "log_interval": 10,
      "eval_interval": 50,
      "verbose": 1,
      "optimization_method": "adam",
      "lora_rank": 8,
      "experiment_name": "Adam_lr1e-5_epoch1"
    }
  },
  {
    "experiment_name": "SGD_lr1e-5_epoch1",
    "train_losses": [
      0.9561558365821838,
      0.8710837364196777,
      0.6053720116615295,
      0.6748582720756531,
      0.9825621247291565,
      0.9491281509399414,
      0.5082895755767822,
      0.5312811136245728,
      1.0506153106689453,
      0.5262417793273926,
      0.6533693075180054,
      0.51222163438797,
      0.5929937362670898,
      0.8417842388153076,
      0.5815098881721497,
      0.6108124852180481,
      0.5641347169876099,
      0.9596732258796692,
      0.6015550494194031,
      0.7078970074653625,
      1.215409755706787,
      0.6343424320220947,
      0.6025338172912598,
      0.868365466594696,
      0.47791025042533875,
      0.8455461263656616,
      0.9437330365180969,
      0.917922854423523,
      0.9180716276168823,
      0.6674965023994446,
      0.9538159370422363,
      0.7184931635856628,
      1.1753334999084473,
      1.0875613689422607,
      0.6891306042671204,
      0.6069987416267395,
      0.8689333200454712,
      0.7840864658355713,
      0.7524279952049255,
      0.7763881683349609,
      0.8852041363716125,
      1.087889313697815,
      0.42815762758255005,
      1.0601162910461426,
      0.6928861141204834,
      0.8853511810302734,
      1.0387873649597168,
      0.6762730479240417,
      0.9914699792861938,
      0.8789538145065308,
      0.725983738899231,
      0.5605022311210632,
      0.41812729835510254,
      0.770461916923523,
      0.709821879863739,
      1.2128349542617798,
      0.7742000818252563,
      0.822475790977478,
      0.790074348449707,
      0.5997791290283203,
      0.7477513551712036,
      0.7758094072341919,
      1.4609663486480713,
      0.7985461354255676,
      0.9219542741775513,
      0.5545997023582458,
      0.4710780382156372,
      0.6453694701194763,
      0.35029399394989014,
      0.33091825246810913,
      0.507311999797821,
      1.0966755151748657,
      0.9912099242210388,
      0.40196454524993896,
      0.8488921523094177,
      0.8006981611251831,
      0.4034474790096283,
      0.7913390398025513,
      0.8958800435066223,
      0.5711827278137207,
      0.47891953587532043,
      0.396911084651947,
      0.5324798822402954,
      0.7704596519470215,
      0.9852069020271301,
      0.9668523669242859,
      0.6648750305175781,
      0.9732451438903809,
      0.6478726267814636,
      0.881127119064331,
      0.9481121301651001,
      1.0265506505966187,
      1.0824333429336548,
      1.2164950370788574,
      0.887357771396637,
      1.1961091756820679,
      0.7587502002716064,
      0.6805620193481445,
      1.1072858572006226,
      0.822719156742096,
      0.6178704500198364,
      0.7303670048713684,
      0.819280743598938,
      0.6574714779853821,
      0.9120354652404785,
      0.696384608745575,
      0.7984724044799805,
      0.9601501822471619,
      0.7029893398284912,
      1.1531107425689697,
      0.5415589213371277,
      0.8386194705963135
    ],
    "train_steps": [
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000,
      1010,
      1020,
      1030,
      1040,
      1050,
      1060,
      1070,
      1080,
      1090,
      1100,
      1110,
      1120
    ],
    "val_losses": [
      0.8007193422621969,
      0.7998021521947707,
      0.7997168175480923,
      0.7991485479187169,
      0.7988441262123392,
      0.7981608432730429,
      0.7979538813672507,
      0.7977287394592945,
      0.7971794423162352,
      0.7968273148204114,
      0.7962631826199342,
      0.7961122373115337,
      0.7958974735793055,
      0.7958144133943471,
      0.7954550799428831,
      0.7953302265970084,
      0.7951819277231258,
      0.7949644910328983,
      0.794804582895605,
      0.7948170882194122,
      0.7945880704286759,
      0.7947193522935766
    ],
    "val_steps": [
      50,
      100,
      150,
      200,
      250,
      300,
      350,
      400,
      450,
      500,
      550,
      600,
      650,
      700,
      750,
      800,
      850,
      900,
      950,
      1000,
      1050,
      1100
    ],
    "args": {
      "model_name": "Qwen/Qwen2.5-Math-1.5B",
      "data_dir": "data",
      "output_dir": "saves/sgd_lr1e-5_epoch1",
      "num_epochs": 1,
      "learning_rate": 1e-05,
      "weight_decay": 0.01,
      "beta1": 0.9,
      "beta2": 0.95,
      "batch_size": 2,
      "grad_accumulation_steps": 4,
      "log_interval": 10,
      "eval_interval": 50,
      "verbose": 1,
      "optimization_method": "sgd",
      "lora_rank": 8,
      "experiment_name": "SGD_lr1e-5_epoch1"
    }
  }
]